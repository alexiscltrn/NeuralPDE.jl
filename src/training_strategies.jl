# struct SamplingLogTargetDensity{D, T, C, S, DER}
#     ps::T
#     residual::C
#     strategy::S
#     lb::NTuple{D, Float64}
#     ub::NTuple{D, Float64}
#     derivative::DER
# end

# function in_bounds(Tar::SamplingLogTargetDensity, θ)
#     vec(sum(Tar.lb .< θ .< Tar.ub, dims=1) .== LogDensityProblems.dimension(Tar))
# end

function in_bound(θ, bound)
    lb = bound[1]
    ub = bound[2]
    vec(sum(lb .< θ .< ub, dims=1) .== length(lb))
end

# residual_pdf(Tar::SamplingLogTargetDensity, θ) = abs2.(vec(Tar.residual(θ, Tar.ps)))

function residual_pdf(θ, residual, ps, k)
    u = get_u()
    f = k == 2 ? x -> abs2.(x) : x -> abs.(x).^k
    return f(vec(u(θ, ps, residual)))
end

# function LogDensityProblems.logdensity(Tar::SamplingLogTargetDensity, θ)
#     res = log.(residual_pdf(Tar, θ) .* Tar.strategy.f_filter(θ) .* in_bounds(Tar, θ))
#     length(res) == 1 ? first(res) : res
# end

# LogDensityProblems.dimension(Tar::SamplingLogTargetDensity) = length(Tar.lb)

# function LogDensityProblems.capabilities(::SamplingLogTargetDensity)
#     LogDensityProblems.LogDensityOrder{0}()
# end

# function get_ℓπ(Tar::SamplingLogTargetDensity)
#     function ℓπ(x)
#         LogDensityProblems.logdensity(Tar, x)
#     end
#     return ℓπ
# end

function get_ℓπ(residual, ps, k, bound, f_filter)

    function ℓπ(x)
        res = log.(residual_pdf(x, residual, ps, k) .* f_filter(x) .* in_bound(x, bound))
        return length(res) == 1 ? first(res) : res
    end

    return ℓπ
end

# function get_∂ℓπ∂θ(Tar::SamplingLogTargetDensity)
#     #Zygote withgradient return nothing for residual function
#     if Tar.derivative != NeuralPDE.numeric_derivative
#         @error "NotImplemented"
#     end

#     # f(x) = LogDensityProblems.logdensity(Tar, x)
#     D = LogDensityProblems.dimension(Tar)
#     u = get_u()

#     function ∂ℓπ∂θ(x::AbstractMatrix)
#         val = LogDensityProblems.logdensity(Tar, x)
#         mapreduce(vcat, 1:D) do d
#             εs = [get_ε(D, d, eltype(Tar.ps), 1)]
#             Tar.derivative(Tar.residual, u, x, εs, 1, Tar.ps)
#         end
#         val, x
#     end

#     return ∂ℓπ∂θ
# end

function get_∂ℓπ∂θ(residual, ps, k, bound, f_filter, derivative = numeric_derivative)

    D = length(bound[1])
    ℓπ = get_ℓπ(residual, ps, k, bound, f_filter)

    function ∂ℓπ∂θ(x::AbstractMatrix)
        val = ℓπ(x)
        mapreduce(vcat, 1:D) do d
            εs = [get_ε(D, d, eltype(ps), 1)]
            derivative(residual, get_u(), x, εs, 1, ps)
        end
        val, x
    end

    return ∂ℓπ∂θ
end

function apply_filter(strategy, pts)
    return pts[:, strategy.f_filter(pts)]
end

"""
an implementation of https://link.springer.com/article/10.1007/s11071-023-08654-w

## Positional Arguments
* `f_filter`: a function that filters the static points generated by the sampling algorithm
* `points`: the number of points that are not moved during training

## Keyword Arguments
* `moving_points`: the number of points that are moved during training (by default, it equals `points`),
* `bcs_points`: the number of points that are used for boundary conditions (by default, it equals `points`),
* `sampling_alg`: the quasi-Monte Carlo sampling algorithm
* `resampling`: if it's false - the full static training set is generated in advance before training,
   and at each iteration, one subset is randomly selected out of the batch.
   If it's true - the static training set isn't generated beforehand, and one set of quasi-random
   points is generated directly at each iteration in runtime. In this case, `minibatch` has no effect,
* `minibatch`: the number of static subsets, if resampling == false.
"""
struct ResidualAdaptiveCollocationPointMovement <: AbstractTrainingStrategy
    points::Int64
    moving_points::Int64
    bcs_points::Int64
    sampling_alg::QuasiMonteCarlo.SamplingAlgorithm
    resampling::Bool
    minibatch::Int64
    n_chains::Int64
    moving_batch::Int64
    resample_moving_every::Int64
    k::Int64
    f_filter::Function
end

function ResidualAdaptiveCollocationPointMovement(points, resample_moving_every; moving_points = points,
    bcs_points = points, sampling_alg = LatinHypercubeSample(), resampling = true,
    minibatch = 0, n_chains = 10, moving_batch = points, k = 1, f_filter = x -> true)
    ResidualAdaptiveCollocationPointMovement(points, moving_points, bcs_points, sampling_alg, resampling, minibatch, n_chains, moving_batch, resample_moving_every, k, f_filter)
end

# Allow the do syntax
function ResidualAdaptiveCollocationPointMovement(f_filter::Function, args...; kwargs...)
    ResidualAdaptiveCollocationPointMovement(args...; kwargs..., f_filter = f_filter)
end

function merge_strategy_with_loss_function(pinnrep::PINNRepresentation,
    strategy::ResidualAdaptiveCollocationPointMovement,
    datafree_pde_loss_function,
    datafree_bc_loss_function)

    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep

    eltypeθ = eltype(pinnrep.flat_init_params)

    bounds = get_bounds(domains, eqs, bcs, eltypeθ, dict_indvars, dict_depvars,
                        strategy)
    pde_bounds, bcs_bounds = bounds

    pde_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy)
                          for (_loss, bound) in zip(datafree_pde_loss_function, pde_bounds)]

    strategy_ = QuasiRandomTraining(strategy.bcs_points;
                                    sampling_alg = strategy.sampling_alg,
                                    resampling = strategy.resampling,
                                    minibatch = strategy.minibatch)
    bc_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy_)
                         for (_loss, bound) in zip(datafree_bc_loss_function, bcs_bounds)]

    pde_loss_functions, bc_loss_functions
end

# function make_density_interp(bound, eltypeθ, p_points, f_filter)
#     lb, ub = bound
#     interp_axes = map(lb, ub) do l, u
#         range(l, u, p_points)
#     end

#     p_coords = reinterpret(reshape, eltypeθ, vec(collect(Iterators.product(interp_axes...))))

#     D = length(lb)
#     P_shape = ntuple(x -> p_points, Val(D))
#     P = convert(Array{eltypeθ, D}, reshape(f_filter(p_coords), P_shape))
#     P /= sum(P)
#     extrapolate(interpolate(tuple(interp_axes...), P, Gridded(Linear())), zero(eltypeθ))
# end

function get_initial_θ(fixed_points, strategy::ResidualAdaptiveCollocationPointMovement)
    indices = findall(==(1), strategy.f_filter(fixed_points))
    kept_indices = Distributions.sample(indices, strategy.n_chains, replace=false)
    return fixed_points[:, Distributions.sample(indices, strategy.n_chains, replace=false)]
end

function residual_sample(n_samples, ϵ, n_steps, init_θ, residual, ps, k, bound, strategy)
    D = length(bound[1])
    f_filter = strategy.f_filter
    metric = UnitEuclideanMetric((D, strategy.n_chains))
    ℓπ = get_ℓπ(residual, ps, k, bound, f_filter)
    ∂ℓπ∂θ = get_∂ℓπ∂θ(residual, ps, k, bound, f_filter)
    hamiltonian = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)
    integrator = Leapfrog(ϵ)
    τ = Trajectory{EndPointTS}(integrator, FixedIntegrationTime(ϵ * n_steps))
    kernel = HMCKernel(τ)

    samples, stats = sample(hamiltonian, kernel, init_θ, n_samples)

    reduce(hcat, samples)
end

function get_loss_function(loss_function, bound, eltypeθ, strategy::ResidualAdaptiveCollocationPointMovement;
    τ = nothing)
    sampling_alg = strategy.sampling_alg
    points = strategy.points
    resampling = strategy.resampling
    minibatch = strategy.minibatch
    k = strategy.k
    n_chains = strategy.n_chains
    f_filter = strategy.f_filter
    resample_moving_every = strategy.resample_moving_every
    moving_points = strategy.moving_points
    moving_batch = strategy.moving_batch

    iteration = 0

    point_batch = nothing
    point_batch = if resampling == false
        batch = generate_quasi_random_points_batch(points, bound, eltypeθ, sampling_alg, minibatch)
    end

    moving_sets = Matrix{eltypeθ}(undef, length(bound[1]), n_chains*moving_points)

    loss = if resampling == true
        θ -> begin
            sets = ChainRulesCore.ignore_derivatives() do
                sets = QuasiMonteCarlo.sample(points, bound[1], bound[2], sampling_alg)
                sets = apply_filter(strategy, sets)

                if iteration % resample_moving_every == 0
                    init_sample = sets[:, Distributions.sample(1:size(sets, 2), n_chains, replace=false)]
                    moving_sets .= residual_sample(moving_points, 0.1, 10, init_sample,
                                                   loss_function, θ, k, bound, strategy)
                end

                moving_sets_ = moving_sets[:,Distributions.sample(1:n_chains*moving_points, moving_batch, replace=false)]

                sets = hcat(sets, moving_sets_)

                adapt(parameterless_type(ComponentArrays.getdata(θ)), sets)
            end

            res = mean(abs2, loss_function(sets, θ))
            iteration += 1
            return res
        end
    else
        θ -> begin
            sets = ChainRulesCore.ignore_derivatives() do
                sets = point_batch[rand(1:minibatch)]
                sets = apply_filter(strategy, sets)

                if iteration % resample_moving_every == 0
                    init_sample = get_initial_θ(sets, strategy)
                    moving_sets .= residual_sample(moving_points, 0.1, 10, init_sample,
                                                   loss_function, θ, k, bound, strategy)
                end

                moving_sets_ = moving_sets[:,Distributions.sample(1:n_chains*moving_points, moving_batch, replace=false)]

                sets = hcat(sets, moving_sets_)

                adapt(parameterless_type(ComponentArrays.getdata(θ)), sets)
            end

            res = mean(abs2, loss_function(sets, θ))
            iteration += 1
            return res
        end
    end
    return loss
end

"""
    FilteredQuasiRandomStrategy(points; bcs_points = points,
                                sampling_alg = LatinHypercubeSample(), resampling = true,
                                minibatch = 0)


A training strategy which uses quasi-Monte Carlo sampling for low discrepancy sequences
that accelerate the convergence in high dimensional spaces over pure random sequences.
This strategy also includes a filter function that can be used to remove points that
do not satisfy certain conditions.

## Positional Arguments
* `f_filter`: a function that filters the points generated by the sampling algorithm
* `points`:  the number of quasi-random points in a sample

## Keyword Arguments

* `bcs_points`: the number of quasi-random points in a sample for boundary conditions
  (by default, it equals `points`),
* `sampling_alg`: the quasi-Monte Carlo sampling algorithm,
* `resampling`: if it's false - the full training set is generated in advance before training,
   and at each iteration, one subset is randomly selected out of the batch.
   Ff it's true - the training set isn't generated beforehand, and one set of quasi-random
   points is generated directly at each iteration in runtime. In this case, `minibatch` has no effect,
* `minibatch`: the number of subsets, if resampling == false.

For more information, see [QuasiMonteCarlo.jl](https://docs.sciml.ai/QuasiMonteCarlo/stable/).
"""

struct FilteredQuasiRandomStrategy <: AbstractTrainingStrategy
    f_filter::Function
    points::Int64
    bcs_points::Int64
    sampling_alg::QuasiMonteCarlo.SamplingAlgorithm
    resampling::Bool
    minibatch::Int64
end

function FilteredQuasiRandomStrategy(f_filter, pde_points, bcs_points=pde_points;
                                     sampling_alg=LatinHypercubeSample(),
                                     resampling=true,
                                     minibatch=0)
    FilteredQuasiRandomStrategy(f_filter, pde_points, bcs_points, sampling_alg, resampling, minibatch)

end

function merge_strategy_with_loss_function(pinnrep::PINNRepresentation,
                                           strategy::FilteredQuasiRandomStrategy,
                                           datafree_pde_loss_function,
                                           datafree_bc_loss_function)
    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep

    eltypeθ = eltype(pinnrep.flat_init_params)

    bounds = get_bounds(domains, eqs, bcs, eltypeθ, dict_indvars, dict_depvars,
                        strategy)
    pde_bounds, bcs_bounds = bounds

    pde_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy)
                          for (_loss, bound) in zip(datafree_pde_loss_function, pde_bounds)]

    strategy_ = QuasiRandomTraining(strategy.bcs_points;
                                    sampling_alg = strategy.sampling_alg,
                                    resampling = strategy.resampling,
                                    minibatch = strategy.minibatch)
    bc_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy_)
                         for (_loss, bound) in zip(datafree_bc_loss_function, bcs_bounds)]

    pde_loss_functions, bc_loss_functions
end

function get_loss_function(loss_function, bound, eltypeθ, strategy::FilteredQuasiRandomStrategy;
                           τ = nothing)
    sampling_alg = strategy.sampling_alg
    points = strategy.points
    resampling = strategy.resampling
    minibatch = strategy.minibatch

    point_batch = nothing
    point_batch = if resampling == false
        batch = generate_quasi_random_points_batch(points, bound, eltypeθ, sampling_alg, minibatch)
    end
    loss = if resampling == true
        θ -> begin
            sets = ChainRulesCore.@ignore_derivatives QuasiMonteCarlo.sample(points,
                                                                             bound[1],
                                                                             bound[2],
                                                                             sampling_alg)

            sets = apply_filter(strategy, sets)
            sets_ = adapt(parameterless_type(ComponentArrays.getdata(θ)), sets)
            mean(abs2, loss_function(sets_, θ))
        end
    else
        θ -> begin
            sets_ = point_batch[rand(1:minibatch)]
            sets_ = apply_filter(strategy, sets_)
            sets__ = adapt(parameterless_type(ComponentArrays.getdata(θ)), sets_)
            mean(abs2, loss_function(sets__, θ))
        end
    end
    return loss
end

"""
    GridTraining(dx)

A training strategy that uses the grid points in a multidimensional grid
with spacings `dx`. If the grid is multidimensional, then `dx` is expected
to be an array of `dx` values matching the dimension of the domain,
corresponding to the grid spacing in each dimension.

## Positional Arguments

* `dx`: the discretization of the grid.
"""
struct GridTraining{T} <: AbstractTrainingStrategy
    dx::T
end

# include dataset points in pde_residual loglikelihood (BayesianPINN)
function merge_strategy_with_loglikelihood_function(pinnrep::PINNRepresentation,
        strategy::GridTraining,
        datafree_pde_loss_function,
        datafree_bc_loss_function; train_sets_pde = nothing,train_sets_bc=nothing)
    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep

    eltypeθ = eltype(pinnrep.flat_init_params)

    # is vec as later each _set in pde_train_sets are columns as points transformed to vector of points (pde_train_sets must be rowwise)
    pde_loss_functions = if !(train_sets_pde isa Nothing)
        pde_train_sets = [train_set[:, 2:end] for train_set in train_sets_pde]
        pde_train_sets = adapt.(parameterless_type(ComponentArrays.getdata(flat_init_params)),
            pde_train_sets)
        [get_loss_function(_loss, _set, eltypeθ, strategy)
                              for (_loss, _set) in zip(datafree_pde_loss_function,
            pde_train_sets)]
    else
        nothing
    end

    bc_loss_functions = if !(train_sets_bc isa Nothing)
        bcs_train_sets = [train_set[:, 2:end] for train_set in train_sets_bc]
        bcs_train_sets = adapt.(parameterless_type(ComponentArrays.getdata(flat_init_params)),
        bcs_train_sets)
        [get_loss_function(_loss, _set, eltypeθ, strategy)
                         for (_loss, _set) in zip(datafree_bc_loss_function, bcs_train_sets)]
    else
        nothing
    end

    pde_loss_functions, bc_loss_functions
end

function merge_strategy_with_loss_function(pinnrep::PINNRepresentation,
                                           strategy::GridTraining,
                                           datafree_pde_loss_function,
                                           datafree_bc_loss_function)
    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep
    dx = strategy.dx
    eltypeθ = eltype(pinnrep.flat_init_params)

    train_sets = generate_training_sets(domains, dx, eqs, bcs, eltypeθ,
                                        dict_indvars, dict_depvars)

    # the points in the domain and on the boundary
    pde_train_sets, bcs_train_sets = train_sets
    pde_train_sets = adapt.(parameterless_type(ComponentArrays.getdata(flat_init_params)),
                            pde_train_sets)
    bcs_train_sets = adapt.(parameterless_type(ComponentArrays.getdata(flat_init_params)),
                            bcs_train_sets)
    pde_loss_functions = [get_loss_function(_loss, _set, eltypeθ, strategy)
                          for (_loss, _set) in zip(datafree_pde_loss_function,
                                                   pde_train_sets)]

    bc_loss_functions = [get_loss_function(_loss, _set, eltypeθ, strategy)
                         for (_loss, _set) in zip(datafree_bc_loss_function, bcs_train_sets)]

    pde_loss_functions, bc_loss_functions
end

function get_loss_function(loss_function, train_set, eltypeθ, strategy::GridTraining;
                           τ = nothing)
    loss = (θ) -> mean(abs2, loss_function(train_set, θ))
end

"""
    StochasticTraining(points; bcs_points = points)

## Positional Arguments

* `points`: number of points in random select training set

## Keyword Arguments

* `bcs_points`: number of points in random select training set for boundary conditions
  (by default, it equals `points`).
"""
struct StochasticTraining <: AbstractTrainingStrategy
    points::Int64
    bcs_points::Int64
end

function StochasticTraining(points; bcs_points = points)
    StochasticTraining(points, bcs_points)
end

function generate_random_points(points, bound, eltypeθ)
    lb, ub = bound
    rand(eltypeθ, length(lb), points) .* (ub .- lb) .+ lb
end

function merge_strategy_with_loss_function(pinnrep::PINNRepresentation,
                                           strategy::StochasticTraining,
                                           datafree_pde_loss_function,
                                           datafree_bc_loss_function)
    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep

    eltypeθ = eltype(pinnrep.flat_init_params)

    bounds = get_bounds(domains, eqs, bcs, eltypeθ, dict_indvars, dict_depvars,
                        strategy)
    pde_bounds, bcs_bounds = bounds

    pde_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy)
                          for (_loss, bound) in zip(datafree_pde_loss_function, pde_bounds)]

    bc_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy)
                         for (_loss, bound) in zip(datafree_bc_loss_function, bcs_bounds)]

    pde_loss_functions, bc_loss_functions
end

function get_loss_function(loss_function, bound, eltypeθ, strategy::StochasticTraining;
                           τ = nothing)
    points = strategy.points
    loss = (θ) -> begin
        sets = generate_random_points(points, bound, eltypeθ)
        sets_ = adapt(parameterless_type(ComponentArrays.getdata(θ)), sets)
        mean(abs2, loss_function(sets_, θ))
    end
    return loss
end

"""
    QuasiRandomTraining(points; bcs_points = points,
                                sampling_alg = LatinHypercubeSample(), resampling = true,
                                minibatch = 0)


A training strategy which uses quasi-Monte Carlo sampling for low discrepancy sequences
that accelerate the convergence in high dimensional spaces over pure random sequences.

## Positional Arguments

* `points`:  the number of quasi-random points in a sample

## Keyword Arguments

* `bcs_points`: the number of quasi-random points in a sample for boundary conditions
  (by default, it equals `points`),
* `sampling_alg`: the quasi-Monte Carlo sampling algorithm,
* `resampling`: if it's false - the full training set is generated in advance before training,
   and at each iteration, one subset is randomly selected out of the batch.
   Ff it's true - the training set isn't generated beforehand, and one set of quasi-random
   points is generated directly at each iteration in runtime. In this case, `minibatch` has no effect,
* `minibatch`: the number of subsets, if resampling == false.

For more information, see [QuasiMonteCarlo.jl](https://docs.sciml.ai/QuasiMonteCarlo/stable/).
"""
struct QuasiRandomTraining <: AbstractTrainingStrategy
    points::Int64
    bcs_points::Int64
    sampling_alg::QuasiMonteCarlo.SamplingAlgorithm
    resampling::Bool
    minibatch::Int64
end

function QuasiRandomTraining(points; bcs_points = points,
                             sampling_alg = LatinHypercubeSample(), resampling = true,
                             minibatch = 0)
    QuasiRandomTraining(points, bcs_points, sampling_alg, resampling, minibatch)
end

function generate_quasi_random_points_batch(points, bound, eltypeθ, sampling_alg,
                                            minibatch)
    lb, ub = bound
    set = QuasiMonteCarlo.generate_design_matrices(points, lb, ub, sampling_alg, minibatch)
    set = map(s -> adapt(parameterless_type(eltypeθ), s), set)
    return set
end

function merge_strategy_with_loss_function(pinnrep::PINNRepresentation,
                                           strategy::QuasiRandomTraining,
                                           datafree_pde_loss_function,
                                           datafree_bc_loss_function)
    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep

    eltypeθ = eltype(pinnrep.flat_init_params)

    bounds = get_bounds(domains, eqs, bcs, eltypeθ, dict_indvars, dict_depvars,
                        strategy)
    pde_bounds, bcs_bounds = bounds

    pde_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy)
                          for (_loss, bound) in zip(datafree_pde_loss_function, pde_bounds)]

    strategy_ = QuasiRandomTraining(strategy.bcs_points;
                                    sampling_alg = strategy.sampling_alg,
                                    resampling = strategy.resampling,
                                    minibatch = strategy.minibatch)
    bc_loss_functions = [get_loss_function(_loss, bound, eltypeθ, strategy_)
                         for (_loss, bound) in zip(datafree_bc_loss_function, bcs_bounds)]

    pde_loss_functions, bc_loss_functions
end

function get_loss_function(loss_function, bound, eltypeθ, strategy::QuasiRandomTraining;
                           τ = nothing)
    sampling_alg = strategy.sampling_alg
    points = strategy.points
    resampling = strategy.resampling
    minibatch = strategy.minibatch

    point_batch = nothing
    point_batch = if resampling == false
        generate_quasi_random_points_batch(points, bound, eltypeθ, sampling_alg, minibatch)
    end
    loss = if resampling == true
        θ -> begin
            sets = ChainRulesCore.@ignore_derivatives QuasiMonteCarlo.sample(points,
                                                                             bound[1],
                                                                             bound[2],
                                                                             sampling_alg)
            sets_ = adapt(parameterless_type(ComponentArrays.getdata(θ)), sets)
            mean(abs2, loss_function(sets_, θ))
        end
    else
        θ -> begin
            sets_ = point_batch[rand(1:minibatch)]
            sets__ = adapt(parameterless_type(ComponentArrays.getdata(θ)), sets_)
            mean(abs2, loss_function(sets__, θ))
        end
    end
    return loss
end

"""
    QuadratureTraining(; quadrature_alg = CubatureJLh(),
                        reltol = 1e-6, abstol = 1e-3,
                        maxiters = 1_000, batch = 100)

A training strategy which treats the loss function as the integral of
||condition|| over the domain. Uses an Integrals.jl algorithm for
computing the (adaptive) quadrature of this loss with respect to the
chosen tolerances, with a batching `batch` corresponding to the maximum
number of points to evaluate in a given integrand call.

## Keyword Arguments

* `quadrature_alg`: quadrature algorithm,
* `reltol`: relative tolerance,
* `abstol`: absolute tolerance,
* `maxiters`: the maximum number of iterations in quadrature algorithm,
* `batch`: the preferred number of points to batch.

For more information on the argument values and algorithm choices, see
[Integrals.jl](https://docs.sciml.ai/Integrals/stable/).
"""
struct QuadratureTraining{Q <: SciMLBase.AbstractIntegralAlgorithm, T} <:
       AbstractTrainingStrategy
    quadrature_alg::Q
    reltol::T
    abstol::T
    maxiters::Int64
    batch::Int64
end

function QuadratureTraining(; quadrature_alg = CubatureJLh(), reltol = 1e-6, abstol = 1e-3,
                            maxiters = 1_000, batch = 100)
    QuadratureTraining(quadrature_alg, reltol, abstol, maxiters, batch)
end

function merge_strategy_with_loss_function(pinnrep::PINNRepresentation,
                                           strategy::QuadratureTraining,
                                           datafree_pde_loss_function,
                                           datafree_bc_loss_function)
    @unpack domains, eqs, bcs, dict_indvars, dict_depvars, flat_init_params = pinnrep
    eltypeθ = eltype(pinnrep.flat_init_params)

    bounds = get_bounds(domains, eqs, bcs, eltypeθ, dict_indvars, dict_depvars,
                        strategy)
    pde_bounds, bcs_bounds = bounds

    lbs, ubs = pde_bounds
    pde_loss_functions = [get_loss_function(_loss, lb, ub, eltypeθ, strategy)
                          for (_loss, lb, ub) in zip(datafree_pde_loss_function, lbs, ubs)]
    lbs, ubs = bcs_bounds
    bc_loss_functions = [get_loss_function(_loss, lb, ub, eltypeθ, strategy)
                         for (_loss, lb, ub) in zip(datafree_bc_loss_function, lbs, ubs)]

    pde_loss_functions, bc_loss_functions
end

function get_loss_function(loss_function, lb, ub, eltypeθ, strategy::QuadratureTraining;
                           τ = nothing)
    if length(lb) == 0
        loss = (θ) -> mean(abs2, loss_function(rand(eltypeθ, 1, 10), θ))
        return loss
    end
    area = eltypeθ(prod(abs.(ub .- lb)))
    f_ = (lb, ub, loss_, θ) -> begin
        # last_x = 1
        function integrand(x, θ)
            # last_x = x
            # mean(abs2,loss_(x,θ), dims=2)
            # size_x = fill(size(x)[2],(1,1))
            x = adapt(parameterless_type(ComponentArrays.getdata(θ)), x)
            sum(abs2, view(loss_(x, θ), 1, :), dims = 2) #./ size_x
        end
        integral_function = BatchIntegralFunction(integrand, max_batch = strategy.batch)
        prob = IntegralProblem(integral_function, (lb, ub), θ)
        solve(prob,
              strategy.quadrature_alg,
              reltol = strategy.reltol,
              abstol = strategy.abstol,
              maxiters = strategy.maxiters)[1]
    end
    loss = (θ) -> 1 / area * f_(lb, ub, loss_function, θ)
    return loss
end

"""
    WeightedIntervalTraining(weights, samples)

A training strategy that generates points for training based on the given inputs.
We split the timespan into equal segments based on the number of weights,
then sample points in each segment based on that segments corresponding weight,
such that the total number of sampled points is equivalent to the given samples

## Positional Arguments

* `weights`: A vector of weights that should sum to 1, representing the proportion of samples at each interval.
* `points`: the total number of samples that we want, across the entire time span

## Limitations

This training strategy can only be used with ODEs (`NNODE`).
"""
struct WeightedIntervalTraining{T} <: AbstractTrainingStrategy
    weights::Vector{T}
    points::Int
end

function WeightedIntervalTraining(weights, points)
    WeightedIntervalTraining(weights, points)
end

function get_loss_function(loss_function, train_set, eltypeθ,
                           strategy::WeightedIntervalTraining;
                           τ = nothing)
    loss = (θ) -> mean(abs2, loss_function(train_set, θ))
    return loss
end
